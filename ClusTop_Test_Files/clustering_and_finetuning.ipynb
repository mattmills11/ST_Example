{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 1: Import Libraries and Set Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and Setup\n",
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import cuda, nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import InputExample, SentenceTransformer, models, losses\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "# Environment Variables\n",
    "FILESPATH = \"/home/tulipan16372/storage_NAS/Misc/Dani_Amaya/sentence-transformers/\"\n",
    "EMBEDDINGS_NAME = \"Matt_embeddings.npy\"\n",
    "CLUSTERS_DATAFRAME_NAME = \"Matt_df_cluster.csv\"  # Fixed name for clustered data CSV\n",
    "TEACHER_MODEL = \"make-multilingual-en-es-2020-10-31_19-04-26\"  # The pretrained model you've been using\n",
    "STUDENT_MODEL = \"make-multilingual-simcse-class\"  # Name to save the fine-tuned student model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 2: Load Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug print to ensure environment variables are set correctly\n",
    "print(f\"FILESPATH: {FILESPATH}\")\n",
    "print(f\"CLUSTERS_DATAFRAME_NAME: {CLUSTERS_DATAFRAME_NAME}\")\n",
    "\n",
    "# Load embeddings\n",
    "embeddings_path = os.path.join(FILESPATH, EMBEDDINGS_NAME)\n",
    "embeddings = np.load(embeddings_path, allow_pickle=True)\n",
    "print(f\"Embeddings loaded from: {embeddings_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 3: UMAP Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP Projection\n",
    "umap_model = UMAP(n_neighbors=15, n_components=2, min_dist=0.0, metric='cosine', init='random', n_jobs=-1)\n",
    "reduced_embeddings = umap_model.fit_transform(embeddings)\n",
    "print(f\"UMAP projection completed. Reduced embeddings shape: {reduced_embeddings.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 4: HDBSCAN Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HDBSCAN Clustering\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=300, min_samples=50, metric='euclidean', cluster_selection_method='eom')\n",
    "hdbscan_model.fit(reduced_embeddings)\n",
    "\n",
    "# Save Clusters\n",
    "df_cluster = pd.DataFrame(reduced_embeddings, columns=[\"umap_x\", \"umap_y\"])\n",
    "df_cluster['cluster'] = hdbscan_model.labels_\n",
    "clusters_dataframe_path = os.path.join(FILESPATH, CLUSTERS_DATAFRAME_NAME)\n",
    "df_cluster.to_csv(clusters_dataframe_path, index=False)\n",
    "print(f\"Clustered data saved to: {clusters_dataframe_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 5: Visualize UMAP Embeddings and HDBSCAN Clusters with Plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "# Load the UMAP-reduced embeddings and cluster labels\n",
    "umap_embeddings_file = os.path.join(FILESPATH, \"Matt_reduced_embeddings.npy\")\n",
    "umap_embeddings = np.load(umap_embeddings_file)\n",
    "\n",
    "clusters_file = os.path.join(FILESPATH, \"Matt_df_cluster.csv\")\n",
    "df_clusters = pd.read_csv(clusters_file)\n",
    "\n",
    "# Extract cluster labels\n",
    "labels = df_clusters['cluster'].values\n",
    "\n",
    "# Create a DataFrame for Plotly visualization\n",
    "df_plot = pd.DataFrame({\n",
    "    'UMAP_1': umap_embeddings[:, 0],\n",
    "    'UMAP_2': umap_embeddings[:, 1],\n",
    "    'Cluster': labels\n",
    "})\n",
    "\n",
    "# Plot with Plotly\n",
    "fig = px.scatter(\n",
    "    df_plot, x='UMAP_1', y='UMAP_2',\n",
    "    color='Cluster',\n",
    "    title='UMAP Projection of Embeddings with HDBSCAN Clusters',\n",
    "    labels={'UMAP_1': 'UMAP Dimension 1', 'UMAP_2': 'UMAP Dimension 2'},\n",
    "    width=800, height=600\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell 6: Fine-tuning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses, models\n",
    "from torch import nn\n",
    "\n",
    "# Set the correct local path for the teacher model\n",
    "TEACHER_MODEL = \"/home/tulipan16372/storage_NAS/Misc/Dani_Amaya/sentence-transformers/make-multilingual-en-es-2020-10-31_19-04-26\"\n",
    "\n",
    "# Environment Variables\n",
    "FILESPATH = \"/home/tulipan16372/storage_NAS/Misc/Dani_Amaya/sentence-transformers/\"\n",
    "CLUSTERS_DATAFRAME_NAME = \"Matt_df_cluster.csv\"\n",
    "\n",
    "# Load Data\n",
    "df_cluster = pd.read_csv(os.path.join(FILESPATH, CLUSTERS_DATAFRAME_NAME))\n",
    "\n",
    "# Extract labels and ensure valid clusters\n",
    "y = df_cluster.loc[df_cluster['cluster'] != -1, \"cluster\"]\n",
    "labels = np.array(y)\n",
    "num_labels = len(set(labels))\n",
    "\n",
    "# Use index as text (since the 'documents' column is not present)\n",
    "texts = df_cluster.index.tolist()\n",
    "\n",
    "# Load the teacher model and prepare for ClusTop fine-tuning\n",
    "word_embedding_model = models.Transformer(TEACHER_MODEL, max_seq_length=128)\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "dense_model = models.Dense(\n",
    "    in_features=pooling_model.get_sentence_embedding_dimension(),\n",
    "    out_features=num_labels,\n",
    "    activation_function=nn.Tanh(),\n",
    ")\n",
    "\n",
    "# Initialize SentenceTransformer model with the components\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model, dense_model])\n",
    "\n",
    "# Prepare training examples\n",
    "train_examples = [\n",
    "    InputExample(texts=[str(text), str(text)], label=int(label))\n",
    "    for text, label in zip(texts, labels) if label != -1\n",
    "]\n",
    "\n",
    "# DataLoader\n",
    "train_dataloader = DataLoader(\n",
    "    train_examples,\n",
    "    shuffle=True,\n",
    "    batch_size=16,\n",
    "    collate_fn=model.smart_batching_collate\n",
    ")\n",
    "\n",
    "# Loss Function\n",
    "train_loss = losses.SoftmaxLoss(\n",
    "    model=model,\n",
    "    sentence_embedding_dimension=model.get_sentence_embedding_dimension(),\n",
    "    num_labels=num_labels\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "print(\"Fine-tuning the model...\")\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    epochs=15,\n",
    "    warmup_steps=100,\n",
    "    show_progress_bar=True,\n",
    "    use_amp=True  # Mixed precision for faster training on GPU\n",
    ")\n",
    "\n",
    "# Save the fine-tuned model\n",
    "student_model_save_path = os.path.join(FILESPATH, \"Matt_ClusTop_fine_tuned_model\")\n",
    "model.save(student_model_save_path)\n",
    "print(f\"Fine-tuned model saved to: {student_model_save_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
